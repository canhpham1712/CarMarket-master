"""
Script scrape d·ªØ li·ªáu Toyota t·ª´ oto.com.vn
Format output gi·ªëng v·ªõi bonbanh.com
"""
import os
import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import random
import csv
from datetime import datetime
from pathlib import Path

# Selenium imports
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    try:
        from webdriver_manager.chrome import ChromeDriverManager
        WEBDRIVER_MANAGER_AVAILABLE = True
    except ImportError:
        WEBDRIVER_MANAGER_AVAILABLE = False
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    WEBDRIVER_MANAGER_AVAILABLE = False
    print("‚ö†Ô∏è Selenium not installed. Install with: pip install selenium webdriver-manager")

# ----------------- C·∫§U H√åNH ----------------- 
OUTPUT_DIR = Path(__file__).resolve().parents[1] / "data"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

MAX_PAGES = 50  # Gi·ªõi h·∫°n s·ªë trang
DELAY_BETWEEN_REQUESTS = (2, 4)  # Delay 2-4 gi√¢y
USE_SELENIUM = True  # S·ª≠ d·ª•ng Selenium ƒë·ªÉ scroll v√† load th√™m listings
SCROLL_PAUSE_TIME = 2  # Th·ªùi gian ch·ªù sau m·ªói l·∫ßn scroll (gi√¢y)
MAX_SCROLL_ATTEMPTS =100  # S·ªë l·∫ßn scroll t·ªëi ƒëa

# URL base cho Toyota tr√™n oto.com.vn
BASE_URL = "https://oto.com.vn/mua-ban-xe-toyota"

# Danh s√°ch c√°c model Toyota
TOYOTA_MODELS = [
    "vios", "innova", "camry", "corolla-altis", "corolla-cross", "yaris", "wigo", "hilux",
    "4-runner", "alphard", "avalon", "avanza", "avanza-premio", "aygo", "corolla", "corona",
    "cressida", "crown", "hiace", "highlander", "innova-cross", "iq", "land-cruiser",
    "land-cruiser-prado", "previa", "raize", "rav4", "rush", "sequoia", "sienna", "veloz",
    "veloz-cross", "venza", "yaris-cross", "zace"
]

# ----------------- H√ÄM H·ªñ TR·ª¢ ----------------- 
def extract_ad_id_from_url(url):
    """T·∫°o ad_id t·ª´ URL oto.com.vn
    Format: /mua-ban-xe-toyota-{model}-{location}/{title}-aidxc{id}
    """
    path = urlparse(url).path
    # T√¨m aidxc{id} trong URL
    match = re.search(r'aidxc(\d+)', path)
    if match:
        return match.group(1)
    # Fallback: l·∫•y ph·∫ßn cu·ªëi c√πng
    ad_id = path.strip('/').split('/')[-1]
    return ad_id

def clean_text(text):
    """L√†m s·∫°ch text"""
    if not text:
        return None
    return ' '.join(str(text).split()).strip()

def extract_price_vnd(text):
    """Tr√≠ch xu·∫•t gi√° theo tri·ªáu VND t·ª´ text (v√≠ d·ª•: "435.000.000 ƒë" -> 435)"""
    if not text:
        return None
    
    text = str(text).lower()
    
    # Pattern 1: "1 t·ª∑ 200 tri·ªáu" ho·∫∑c "1 t·ª∑ 420 tri·ªáu"
    match = re.search(r'(\d+(?:\.\d+)?)\s*t·ª∑\s*(\d+)?\s*tri·ªáu?', text)
    if match:
        ty = float(match.group(1))
        tr = int(match.group(2)) if match.group(2) else 0
        return int(ty * 1000 + tr)
    
    # Pattern 2: "1.42 t·ª∑" ho·∫∑c "1 t·ª∑"
    match = re.search(r'(\d+(?:\.\d+)?)\s*t·ª∑', text)
    if match:
        return int(float(match.group(1)) * 1000)
    
    # Pattern 3: "250 tri·ªáu" ho·∫∑c "420 tri·ªáu"
    match = re.search(r'(\d+)\s*tri·ªáu', text)
    if match:
        return int(match.group(1))
    
    # Pattern 4: "435.000.000 ƒë" ho·∫∑c "500.000.000"
    text_clean = text.replace('.', '').replace(',', '').replace(' ', '').replace('ƒë', '')
    match = re.search(r'(\d+)', text_clean)
    if match:
        price_vnd = int(match.group(1))
        price_million = price_vnd // 1000000
        return price_million if price_million > 0 else None
    
    return None

def extract_mileage_from_text(text):
    """Tr√≠ch xu·∫•t s·ªë km t·ª´ text, tr·∫£ v·ªÅ format "XXXXX km" """
    if not text:
        return None
    
    text = str(text).lower()
    
    # Pattern 1: "5 v·∫°n km" ho·∫∑c "5.5 v·∫°n km" -> 50000 km
    match_van = re.search(r'([\d,\.]+)\s*v·∫°n\s*km', text)
    if match_van:
        num_str = match_van.group(1).replace(',', '.').replace(' ', '')
        try:
            num = float(num_str) * 10000
            return f"{int(num)} km"
        except:
            pass
    
    # Pattern 2: "50,000 km" ho·∫∑c "50.000 km" ho·∫∑c "50000 km"
    match_km = re.search(r'([\d,\.]+)\s*km', text)
    if match_km:
        num_str = match_km.group(1).replace(',', '').replace('.', '')
        try:
            return f"{int(num_str)} km"
        except:
            pass
    
    # Pattern 3: Ch·ªâ c√≥ s·ªë (kh√¥ng c√≥ "km") - gi·∫£ ƒë·ªãnh l√† km n·∫øu s·ªë l·ªõn h∆°n 1000
    match_num = re.search(r'(\d{4,})', text)
    if match_num:
        num_str = match_num.group(1)
        try:
            num = int(num_str)
            if num >= 1000:
                return f"{num} km"
        except:
            pass
    
    return None

def parse_color_from_text(text):
    """Parse m√†u s·∫Øc t·ª´ text"""
    if not text:
        return None
    
    text_lower = str(text).lower()
    color_map = {
        'tr·∫Øng': 'Tr·∫Øng',
        'ƒëen': 'ƒêen',
        'b·∫°c': 'B·∫°c',
        'x√°m': 'X√°m',
        'ghi': 'Ghi',
        'ƒë·ªè': 'ƒê·ªè',
        'xanh': 'Xanh',
        'v√†ng': 'V√†ng',
        'c√°t': 'C√°t',
        'n√¢u': 'N√¢u',
        'b·∫°ch kim': 'B·∫°ch kim',
        'xanh d∆∞∆°ng': 'Xanh d∆∞∆°ng',
        'xanh l√°': 'Xanh l√°',
    }
    
    for key, value in color_map.items():
        if key in text_lower:
            return value
    
    return None

def extract_version_from_title(title, make, model):
    """
    Extract version t·ª´ title oto.com.vn
    Format: "Toyota Vios 1.5 G CVT 2015" -> "1.5 G CVT"
    """
    if not title or not make or not model:
        return None
    
    title = str(title).strip()
    make_str = str(make).strip()
    model_str = str(model).strip()
    
    # Pattern 1: Make Model Version Year (oto.com.vn format)
    # V√≠ d·ª•: "Toyota Vios 1.5 G CVT 2015"
    pattern = rf"{re.escape(make_str)}\s+{re.escape(model_str)}\s+(.+?)\s+(20\d{{2}})"
    match = re.search(pattern, title, re.IGNORECASE)
    
    if match:
        version = match.group(1).strip()
        # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng ph·∫£i version ·ªü cu·ªëi
        version = re.sub(r'\s*-\s*$', '', version)
        # Lo·∫°i b·ªè c√°c t·ª´ th√¥ng th∆∞·ªùng kh√¥ng ph·∫£i version
        version = re.sub(r'\s+(Japan|ch√≠nh ch·ªß|cavet|ch·ªß|m√†u|xe|ch·ªó|s·ªë|t·ª± ƒë·ªông|ch·∫°y|xƒÉng|km|dep|cam|ket).*$', '', version, flags=re.IGNORECASE)
        version = version.strip()
        
        # Ch·ªâ tr·∫£ v·ªÅ n·∫øu c√≥ ch·ª©a s·ªë ho·∫∑c l√† version h·ª£p l·ªá (nh∆∞ "G", "GL", "E", "S")
        if version:
            # N·∫øu c√≥ s·ªë ho·∫∑c l√† ch·ªØ c√°i ƒë∆°n (nh∆∞ G, GL, E, S)
            if re.search(r'\d', version) or (len(version) <= 5 and version.isupper()):
                return version
    
    # Pattern 2: T√¨m pattern s·ªë + ch·ªØ (nh∆∞ "1.5G", "2.5G", "1.5 E")
    if make_str.lower() in title.lower() and model_str.lower() in title.lower():
        # T√¨m pattern: s·ªë.ch·ªØ ho·∫∑c s·ªë ch·ªØ (nh∆∞ "1.5G", "2.5G", "1.5 E", "1.5 G CVT")
        pattern = r'([\d\.]+\s*[A-Z]+(?:\s+[A-Z]+)*)'
        matches = re.findall(pattern, title)
        for match in matches:
            version_candidate = match.strip()
            # Ki·ªÉm tra xem c√≥ ph·∫£i version kh√¥ng (c√≥ s·ªë v√† ch·ªØ, ƒë·ªô d√†i h·ª£p l√Ω)
            if re.search(r'\d', version_candidate) and len(version_candidate) <= 20:
                # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng ph·∫£i version
                version_candidate = re.sub(r'\s+(Japan|ch√≠nh ch·ªß|cavet|ch·ªß|m√†u|xe|ch·ªó|s·ªë|t·ª± ƒë·ªông|ch·∫°y|xƒÉng|km).*$', '', version_candidate, flags=re.IGNORECASE)
                version_candidate = version_candidate.strip()
                if version_candidate:
                    return version_candidate
    
    return None

def get_car_details(url, headers):
    """L·∫•y chi ti·∫øt 1 xe t·ª´ trang chi ti·∫øt oto.com.vn"""
    try:
        res = requests.get(url, timeout=15, headers=headers)
        res.raise_for_status()
        # ƒê·∫£m b·∫£o encoding UTF-8
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, "html.parser")
        
        # Title - t·ª´ h1.title-detail
        title = None
        h1_tag = soup.find("h1", class_="title-detail")
        if h1_tag:
            title = clean_text(h1_tag.get_text())
        
        # Make - m·∫∑c ƒë·ªãnh Toyota
        make = "Toyota"
        
        # Model - extract t·ª´ title ho·∫∑c URL
        model = None
        if title:
            # Format: "Toyota Zace 2004" -> "Zace"
            title_parts = title.split()
            if len(title_parts) >= 2 and title_parts[0].lower() == "toyota":
                model = title_parts[1]
        
        # Version - t·ª´ title
        version = None
        if title and make and model:
            version = extract_version_from_title(title, make, model)
        
        # Price - t·ª´ span.price trong box-price ho·∫∑c hidden input
        price_vnd = None
        # Th·ª≠ t·ª´ hidden input tr∆∞·ªõc (ch√≠nh x√°c h∆°n)
        price_input = soup.find("input", id="hddPrice")
        if price_input and price_input.get("value"):
            try:
                price_value = int(price_input.get("value"))
                price_vnd = price_value // 1000000  # Chuy·ªÉn t·ª´ VND sang tri·ªáu
            except:
                pass
        
        # N·∫øu kh√¥ng c√≥ t·ª´ hidden input, l·∫•y t·ª´ span.price
        if not price_vnd:
            price_tag = soup.find("div", class_="box-price")
            if price_tag:
                price_span = price_tag.find("span", class_="price")
                if price_span:
                    price_text = clean_text(price_span.get_text())
                    price_vnd = extract_price_vnd(price_text)
        
        # Th√¥ng tin t·ª´ hidden inputs (ch√≠nh x√°c h∆°n)
        year_input = soup.find("input", id="hddYear")
        year = None
        if year_input and year_input.get("value"):
            year = year_input.get("value")
        
        # Seats t·ª´ hidden input
        seats_input = soup.find("input", id="numberOfSeat")
        seats = None
        if seats_input and seats_input.get("value"):
            seats_value = seats_input.get("value")
            if seats_value and seats_value != "0":
                seats = seats_value
        
        # Fuel type t·ª´ hidden input
        fuel_type_input = soup.find("input", id="fuelType")
        fuel = None
        if fuel_type_input and fuel_type_input.get("value"):
            fuel_type_value = fuel_type_input.get("value")
            fuel_map = {"1": "XƒÉng", "2": "D·∫ßu", "3": "ƒêi·ªán", "4": "Hybrid"}
            fuel = fuel_map.get(fuel_type_value)
        
        # Origin t·ª´ hidden input
        made_in_input = soup.find("input", id="madeInBy")
        origin = None
        if made_in_input and made_in_input.get("value"):
            made_in_value = made_in_input.get("value")
            origin_map = {"1": "Vi·ªát Nam", "2": "Nh·∫≠t B·∫£n", "3": "Th√°i Lan", "4": "H√†n Qu·ªëc"}
            origin = origin_map.get(made_in_value)
        
        # Body t·ª´ hidden input
        classification_input = soup.find("input", id="classificationName")
        body = None
        if classification_input and classification_input.get("value"):
            body = classification_input.get("value")
        
        # Th√¥ng tin t·ª´ box-info-detail (fallback n·∫øu hidden inputs kh√¥ng c√≥)
        info_box = soup.find("div", class_="box-info-detail")
        mileage = None
        gearbox = None
        location = None
        
        if info_box:
            # T√¨m t·∫•t c·∫£ c√°c list-info
            info_lists = info_box.find_all("ul", class_="list-info")
            for info_list in info_lists:
                items = info_list.find_all("li")
                for item in items:
                    label_elem = item.find("label", class_="label")
                    if not label_elem:
                        continue
                    
                    label_text = clean_text(label_elem.get_text()).lower()
                    # L·∫•y text sau label - c√°ch t·ªët h∆°n
                    value_text = None
                    # Th·ª≠ l·∫•y t·ª´ div.small tr∆∞·ªõc (cho location, t√¨nh tr·∫°ng)
                    small_div = item.find("div", class_="small")
                    if small_div:
                        value_text = clean_text(small_div.get_text())
                    else:
                        # L·∫•y text node tr·ª±c ti·∫øp sau label element
                        # T√¨m text node ngay sau label
                        next_sibling = label_elem.next_sibling
                        if next_sibling:
                            if isinstance(next_sibling, str):
                                value_text = clean_text(next_sibling)
                            else:
                                # N·∫øu l√† element, l·∫•y text c·ªßa n√≥
                                value_text = clean_text(next_sibling.get_text())
                        
                        # Fallback: L·∫•y to√†n b·ªô text c·ªßa li, r·ªìi lo·∫°i b·ªè label
                        if not value_text or not value_text.strip():
                            item_text = clean_text(item.get_text())
                            label_text_full = clean_text(label_elem.get_text())
                            # Lo·∫°i b·ªè label text v√† d·∫•u ":"
                            value_text = item_text.replace(label_text_full, "").replace(":", "").strip()
                    
                    # NƒÉm SX (fallback n·∫øu kh√¥ng c√≥ t·ª´ hidden input)
                    if 'nƒÉm sx' in label_text and not year:
                        year_match = re.search(r'20\d{2}', value_text)
                        if year_match:
                            year = year_match.group()
                    
                    # Nhi√™n li·ªáu (fallback)
                    if 'nhi√™n li·ªáu' in label_text and not fuel:
                        fuel_text = value_text.lower()
                        if 'xƒÉng' in fuel_text or 'm√°y xƒÉng' in fuel_text:
                            fuel = "XƒÉng"
                        elif 'd·∫ßu' in fuel_text or 'diesel' in fuel_text:
                            fuel = "D·∫ßu"
                        elif 'ƒëi·ªán' in fuel_text or 'electric' in fuel_text:
                            fuel = "ƒêi·ªán"
                        elif 'hybrid' in fuel_text:
                            fuel = "Hybrid"
                    
                    # Ki·ªÉu d√°ng (fallback)
                    if 'ki·ªÉu d√°ng' in label_text and not body:
                        body_text = value_text.lower()
                        if 'sedan' in body_text:
                            body = "Sedan"
                        elif 'suv' in body_text:
                            body = "SUV"
                        elif 'hatchback' in body_text:
                            body = "Hatchback"
                        elif 'crossover' in body_text:
                            body = "Crossover"
                        elif 'mpv' in body_text or 'van' in body_text or 'minivan' in body_text:
                            body = "MPV"
                        elif 'pickup' in body_text or 'b√°n t·∫£i' in body_text:
                            body = "Pickup"
                    
                    # Km ƒë√£ ƒëi
                    if ('km ƒë√£ ƒëi' in label_text or 'km ƒëi' in label_text) and not mileage:
                        if value_text:
                            mileage = extract_mileage_from_text(value_text)
                    
                    # H·ªôp s·ªë
                    if 'h·ªôp s·ªë' in label_text and not gearbox:
                        if value_text:
                            gearbox_text = value_text.lower()
                            if 't·ª± ƒë·ªông' in gearbox_text or 'automatic' in gearbox_text or 'cvt' in gearbox_text or 's·ªë t·ª± ƒë·ªông' in gearbox_text:
                                gearbox = "T·ª± ƒë·ªông"
                            elif 's·ªë s√†n' in gearbox_text or 's·ªë tay' in gearbox_text or 'manual' in gearbox_text or 'mt' in gearbox_text:
                                gearbox = "S·ªë s√†n"
                            else:
                                # N·∫øu kh√¥ng match, l·∫•y nguy√™n value_text
                                gearbox = value_text
                    
                    # Xu·∫•t x·ª© (fallback)
                    if 'xu·∫•t x·ª©' in label_text and not origin:
                        origin_text = value_text.lower()
                        if 'nh·∫≠t' in origin_text or 'japan' in origin_text:
                            origin = "Nh·∫≠t B·∫£n"
                        elif 'th√°i' in origin_text or 'thailand' in origin_text:
                            origin = "Th√°i Lan"
                        elif 'h√†n' in origin_text or 'korea' in origin_text:
                            origin = "H√†n Qu·ªëc"
                        elif 'vi·ªát' in origin_text or 'vietnam' in origin_text or 'trong n∆∞·ªõc' in origin_text:
                            origin = "Vi·ªát Nam"
                    
                    # T·ªânh th√†nh
                    if 't·ªânh th√†nh' in label_text and not location:
                        if value_text:
                            location = value_text
        
        # Engine - k·∫øt h·ª£p fuel
        engine = fuel if fuel else None
        
        # Engine power - kh√¥ng th·∫•y trong HTML m·∫´u
        engine_power = None
        
        # Description - t·ª´ div.description
        description = None
        desc_div = soup.find("div", class_="description")
        if desc_div:
            # L·∫•y text v√† ƒë·∫£m b·∫£o encoding UTF-8
            desc_text = desc_div.get_text()
            if desc_text:
                # Clean v√† normalize text
                description = clean_text(desc_text)
                # ƒê·∫£m b·∫£o l√† string UTF-8
                if isinstance(description, bytes):
                    description = description.decode('utf-8', errors='ignore')
        
        # Color - t·ª´ description ho·∫∑c title
        color = None
        if description:
            color = parse_color_from_text(description.lower())
        if not color and title:
            color = parse_color_from_text(title.lower())
        
        # Accident free & single owner t·ª´ description
        accident_free = None
        single_owner = False
        
        full_text = (description or '').lower() + ' ' + (title or '').lower()
        
        if any(kw in full_text for kw in ['kh√¥ng tai n·∫°n', 'ko tai n·∫°n', 'kh√¥ng ƒë√¢m ƒë·ª•ng', 'ko ƒë√¢m', 'ch∆∞a ƒë√¢m', 'accident free', 'cam k·∫øt kh√¥ng tai n·∫°n']):
            accident_free = True
        elif any(kw in full_text for kw in ['tai n·∫°n', 'ƒë√¢m ƒë·ª•ng', 'accident']):
            accident_free = False
        
        single_owner = any(kw in full_text for kw in ['1 ch·ªß', 'm·ªôt ch·ªß', 'single owner', 'ch·ªß duy nh·∫•t', 'ch·ªß t·ª´ ƒë·∫ßu', 'ch·ªß t·ª´ m·ªõi', 'ch√≠nh ch·ªß'])
        
        return {
            "title": title,
            "make": make,
            "model": model,
            "version": version,
            "price_vnd": price_vnd,
            "mileage": mileage,
            "location": location,
            "year": year,
            "fuel": fuel,
            "engine": engine,
            "gearbox": gearbox,
            "body": body,
            "color": color,
            "seats": seats,
            "engine_power": engine_power,
            "origin": origin,
            "accident_free": accident_free,
            "single_owner": single_owner,
            "description": description,
            "url": url
        }
    except Exception as e:
        print(f"  ‚ùå Error fetching {url}: {e}")
        return None

def get_all_listings_with_selenium(url, max_scroll_attempts=MAX_SCROLL_ATTEMPTS):
    """
    S·ª≠ d·ª•ng Selenium ƒë·ªÉ scroll v√† load th√™m listings
    URL s·∫Ω t·ª± ƒë·ªông thay ƒë·ªïi th√†nh /p2, /p3, ... khi scroll
    """
    if not SELENIUM_AVAILABLE:
        print("  ‚ö†Ô∏è Selenium not available, falling back to requests only")
        return []
    
    all_links = set()
    
    try:
        # Setup Chrome options
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Ch·∫°y ·∫©n browser
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # S·ª≠ d·ª•ng webdriver-manager ƒë·ªÉ t·ª± ƒë·ªông t·∫£i ChromeDriver
        if WEBDRIVER_MANAGER_AVAILABLE:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
        else:
            # Fallback: s·ª≠ d·ª•ng ChromeDriver t·ª´ PATH
            driver = webdriver.Chrome(options=chrome_options)
        
        driver.set_page_load_timeout(30)
        
        print(f"  üåê Loading page with Selenium: {url}")
        driver.get(url)
        time.sleep(3)  # ƒê·ª£i page load
        
        # L·∫•y links ban ƒë·∫ßu
        initial_links = extract_links_from_page(driver.page_source)
        all_links.update(initial_links)
        print(f"  üìä Initial links: {len(initial_links)}")
        
        # Scroll v√† load th√™m
        last_count = len(all_links)
        scroll_attempts = 0
        no_new_links_count = 0
        
        while scroll_attempts < max_scroll_attempts:
            # Scroll xu·ªëng cu·ªëi trang
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(SCROLL_PAUSE_TIME)
            
            # Ki·ªÉm tra URL c√≥ thay ƒë·ªïi kh√¥ng (c√≥ th·ªÉ th√†nh /p2, /p3, etc.)
            current_url = driver.current_url
            print(f"  üìç Current URL: {current_url}")
            
            # L·∫•y links m·ªõi
            new_links = extract_links_from_page(driver.page_source)
            all_links.update(new_links)
            
            new_count = len(all_links)
            if new_count > last_count:
                print(f"  ‚úÖ Found {new_count - last_count} new links (total: {new_count})")
                last_count = new_count
                no_new_links_count = 0
            else:
                no_new_links_count += 1
                if no_new_links_count >= 3:
                    print(f"  ‚úÖ No new links after {no_new_links_count} scrolls, stopping")
                    break
            
            scroll_attempts += 1
        
        driver.quit()
        print(f"  üéØ Total unique links found: {len(all_links)}")
        return list(all_links)
        
    except Exception as e:
        print(f"  ‚ùå Selenium error: {str(e)[:200]}")
        if 'driver' in locals():
            try:
                driver.quit()
            except:
                pass
        return []

def extract_links_from_page(html_content):
    """Extract car links t·ª´ HTML content"""
    soup = BeautifulSoup(html_content, "html.parser")
    links = []
    
    # T√¨m trong div.item-car
    item_cars = soup.find_all("div", class_="item-car")
    for item_car in item_cars:
        link = None
        title_h3 = item_car.find("h3", class_="title")
        if title_h3:
            link = title_h3.find("a", href=True)
        
        if not link:
            photo_div = item_car.find("div", class_="photo")
            if photo_div:
                link = photo_div.find("a", href=True)
        
        if not link:
            link = item_car.find("a", href=True)
        
        if link:
            href = link.get("href")
            if href:
                if not href.startswith('http'):
                    href = urljoin('https://oto.com.vn', href)
                
                if '/mua-ban-xe-toyota' in href and 'aidxc' in href:
                    href = href.split('#')[0]
                    if href not in links:
                        links.append(href)
    
    return links

def try_direct_pagination(url_base, max_pages=MAX_PAGES):
    """
    Th·ª≠ truy c·∫≠p tr·ª±c ti·∫øp c√°c URL /p2, /p3, ... 
    URL format: https://oto.com.vn/mua-ban-xe-toyota-{model}/p2
    """
    all_links = set()
    
    for page in range(1, max_pages + 1):
        if page == 1:
            url = url_base
        else:
            # Th·ª≠ format /p2, /p3, etc.
            url = f"{url_base}/p{page}"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            res = requests.get(url, timeout=15, headers=headers)
            res.raise_for_status()
            res.encoding = 'utf-8'
            
            links = extract_links_from_page(res.text)
            if not links:
                print(f"  ‚ö†Ô∏è No links found on page {page}, stopping pagination")
                break
            
            all_links.update(links)
            print(f"  üìÑ Page {page}: Found {len(links)} links (total: {len(all_links)})")
            
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error accessing page {page}: {str(e)[:100]}")
            break
    
    return list(all_links)

def scrape_oto_listings(model=None, max_pages=MAX_PAGES, scraped_urls=None):
    """Scrape listings t·ª´ oto.com.vn cho m·ªôt model c·ª• th·ªÉ ho·∫∑c t·∫•t c·∫£ models"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    cars = []
    if scraped_urls is None:
        scraped_urls = set()
    
    # X√°c ƒë·ªãnh danh s√°ch models ƒë·ªÉ scrape
    if model:
        models_to_scrape = [model]
    else:
        models_to_scrape = TOYOTA_MODELS
    
    for model_name in models_to_scrape:
        model_url = f"https://oto.com.vn/mua-ban-xe-toyota-{model_name}"
        
        print(f"\n{'='*70}")
        print(f"üöó Model: {model_name}")
        print(f"{'='*70}")
        
        # Oto.com.vn s·ª≠ d·ª•ng infinite scroll v·ªõi URL thay ƒë·ªïi th√†nh /p2, /p3, ...
        # Th·ª≠ 2 c√°ch:
        # 1. Truy c·∫≠p tr·ª±c ti·∫øp /p2, /p3, ... (n·∫øu ho·∫°t ƒë·ªông)
        # 2. S·ª≠ d·ª•ng Selenium ƒë·ªÉ scroll v√† load th√™m
        url_page = model_url
        print(f"\nüìÑ Scraping: {url_page}")
        
        car_links = []
        
        # C√°ch 1: Th·ª≠ truy c·∫≠p tr·ª±c ti·∫øp c√°c URL /p2, /p3, ...
        print(f"  üîç Trying direct pagination (/p2, /p3, ...)...")
        direct_links = try_direct_pagination(url_page, max_pages=max_pages)
        if direct_links:
            car_links.extend(direct_links)
            print(f"  ‚úÖ Direct pagination found {len(direct_links)} links")
        
        # C√°ch 2: S·ª≠ d·ª•ng Selenium ƒë·ªÉ scroll (n·∫øu ƒë∆∞·ª£c b·∫≠t v√† Selenium available)
        if USE_SELENIUM and SELENIUM_AVAILABLE:
            print(f"  üîç Using Selenium to scroll and load more...")
            selenium_links = get_all_listings_with_selenium(url_page, max_scroll_attempts=MAX_SCROLL_ATTEMPTS)
            if selenium_links:
                car_links.extend(selenium_links)
                print(f"  ‚úÖ Selenium found {len(selenium_links)} links")
        
        # C√°ch 3: Fallback - ch·ªâ l·∫•y t·ª´ trang ƒë·∫ßu ti√™n
        if not car_links:
            print(f"  üîç Fallback: Getting links from first page only...")
            try:
                res = requests.get(url_page, timeout=15, headers=headers)
                res.raise_for_status()
                res.encoding = 'utf-8'
                car_links = extract_links_from_page(res.text)
                print(f"  ‚úÖ Found {len(car_links)} links from first page")
            except Exception as e:
                print(f"  ‚ùå Error getting first page: {str(e)[:100]}")
        
        # Lo·∫°i b·ªè duplicates
        car_links = list(set(car_links))
        
        if not car_links:
            print(f"  ‚ö†Ô∏è No cars found for {model_name}")
            continue
        
        print(f"  üîç Total unique car links: {len(car_links)}")
        
        try:
            for idx, url_full in enumerate(car_links, 1):
                # Ki·ªÉm tra duplicate
                if url_full in scraped_urls:
                    continue
                scraped_urls.add(url_full)
                
                ad_id = extract_ad_id_from_url(url_full)
                
                # L·∫•y chi ti·∫øt xe
                details = get_car_details(url_full, headers)
                if not details:
                    continue
                
                # Ch·ªâ l·∫•y Toyota
                if details.get("make") and details["make"].lower() != "toyota":
                    continue
                
                car_data = {
                    "ad_id": ad_id,
                    "make": details.get("make") or "Toyota",
                    "model": details.get("model"),
                    "version": details.get("version"),
                    "title": details["title"],
                    "price_vnd": details["price_vnd"],
                    "mileage": details["mileage"],
                    "location": details["location"],
                    "year": details["year"],
                    "fuel": details["fuel"],
                    "engine": details.get("engine"),
                    "gearbox": details["gearbox"],
                    "body": details["body"],
                    "color": details["color"],
                    "seats": details["seats"],
                    "engine_power": details["engine_power"],
                    "origin": details["origin"],
                    "accident_free": details["accident_free"],
                    "single_owner": details["single_owner"],
                    "description": details["description"],
                    "url": details["url"]
                }
                
                cars.append(car_data)
                title_short = (details['title'][:50] + '...') if details['title'] and len(details['title']) > 50 else details['title']
                print(f"  ‚úÖ [{len(cars)}] {title_short} | {details['price_vnd']} tri·ªáu")
                
                # Delay gi·ªØa c√°c request
                time.sleep(random.uniform(*DELAY_BETWEEN_REQUESTS))
                
        except Exception as e:
            error_msg = str(e)[:200]
            print(f"  ‚ùå Error scraping {model_name}: {error_msg}")
            import traceback
            traceback.print_exc()
            continue
        
        # Delay gi·ªØa c√°c model
        if model_name != models_to_scrape[-1]:
            delay = random.uniform(5, 8)
            print(f"‚è≥ Waiting {delay:.1f}s before next model...")
            time.sleep(delay)
    
    return cars

def main():
    """Main function"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = OUTPUT_DIR / f"toyota_oto_{timestamp}.csv"
    
    fieldnames = [
        "ad_id", "make", "model", "version", "title", "price_vnd", "mileage", "location",
        "year", "fuel", "engine", "gearbox", "body", "color", "seats", "engine_power",
        "origin", "accident_free", "single_owner", "description", "url"
    ]
    
    print(f"\nüöÄ Starting Oto.com.vn scraper for Toyota")
    print(f"üìã Models: {len(TOYOTA_MODELS)} models")
    print(f"üìã Max pages per model: {MAX_PAGES}")
    print(f"üíæ Output file: {csv_filename}\n")
    
    cars = scrape_oto_listings(model=None, max_pages=MAX_PAGES)
    
    if cars:
        with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(cars)
        
        print(f"\n{'='*60}")
        print(f"üéâ SCRAPING COMPLETED!")
        print(f"{'='*60}")
        print(f"üìä Total cars scraped: {len(cars)}")
        print(f"üíæ Saved to: {csv_filename}")
        
        # Th·ªëng k√™
        from collections import Counter
        model_counts = Counter(car.get("model") or "Unknown" for car in cars)
        print(f"\nüìà Breakdown by model:")
        for model, count in model_counts.most_common():
            print(f"  - {model}: {count} cars")
    else:
        print("\n‚ö†Ô∏è No cars scraped!")

if __name__ == "__main__":
    main()

