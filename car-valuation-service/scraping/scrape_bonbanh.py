import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import random
import csv
from datetime import datetime

# ----------------- Cáº¤U HÃŒNH ----------------- 
BRANDS = ["toyota", "vinfast", "honda", "hyundai", "kia", "mazda", "suzuki", "bmw", "ford", "mercedes-benz"]
OUTPUT_DIR = "car_data"
MAX_PAGES_PER_MODEL = 5

# Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ----------------- HÃ€M Há»– TRá»¢ ----------------- 
def extract_ad_id_from_url(url):
    """Táº¡o ad_id tá»« URL"""
    path = urlparse(url).path
    ad_id = path.strip('/').split('/')[-1]
    return ad_id

def clean_text(text):
    """LÃ m sáº¡ch text, loáº¡i bá» khoáº£ng tráº¯ng thá»«a"""
    if not text:
        return None
    return ' '.join(text.split()).strip()

def extract_price(text):
    """TrÃ­ch xuáº¥t giÃ¡ theo triá»‡u VND tá»« text"""
    if not text:
        return None
    text = text.lower().replace('.', '').replace(',', '')
    
    # Xá»­ lÃ½ pattern "1 tá»· 420 triá»‡u"
    match = re.search(r'(\d+(?:\.\d+)?)\s*tá»·\s*(\d+)?\s*triá»‡u?', text)
    if match:
        ty = float(match.group(1))
        tr = int(match.group(2)) if match.group(2) else 0
        return int(ty * 1000 + tr)
    
    # Xá»­ lÃ½ "1.42 tá»·"
    match = re.search(r'(\d+(?:\.\d+)?)\s*tá»·', text)
    if match:
        return int(float(match.group(1)) * 1000)
    
    # Xá»­ lÃ½ "420 triá»‡u"
    match = re.search(r'(\d+)\s*triá»‡u', text)
    if match:
        return int(match.group(1))
    
    return None

def extract_mileage(text):
    """TrÃ­ch xuáº¥t sá»‘ km tá»« text, xá»­ lÃ½ cáº£ 'váº¡n km' vÃ  'km' thÆ°á»ng"""
    if not text:
        return None
    text = str(text).lower()
    
    # Pattern 1: "5 váº¡n km" hoáº·c "5.5 váº¡n km" -> 50000 km hoáº·c 55000 km
    match_van = re.search(r'([\d,\.]+)\s*váº¡n\s*km', text)
    if match_van:
        num_str = match_van.group(1).replace(',', '.').replace(' ', '')
        try:
            num = float(num_str) * 10000  # 1 váº¡n = 10,000
            return f"{int(num)} km"
        except:
            pass
    
    # Pattern 2: "50,000 km" hoáº·c "50.000 km" -> 50000 km
    match_km = re.search(r'([\d,\.]+)\s*km', text)
    if match_km:
        num_str = match_km.group(1).replace(',', '').replace('.', '')
        try:
            return f"{int(num_str)} km"
        except:
            pass
    
    return None

def get_car_details(url, headers):
    """Láº¥y chi tiáº¿t 1 xe tá»« trang chi tiáº¿t"""
    try:
        res = requests.get(url, timeout=15, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        
        # Title - láº¥y tá»« h1 hoáº·c title
        title = None
        extracted_make = None
        extracted_model = None
        
        h1_tag = soup.find("h1", class_="car-title")
        if not h1_tag:
            h1_tag = soup.find("h1")
        if h1_tag:
            title = clean_text(h1_tag.get_text())
        else:
            title_tag = soup.find("title")
            if title_tag:
                title = clean_text(title_tag.get_text())
        
        # Extract make vÃ  model tá»« title (format: "Xe <Make> <Model> <Version> <Year> - <Price>")
        if title:
            title_match = re.match(r'Xe\s+(\w+)\s+([^\s]+)', title, re.I)
            if title_match:
                extracted_make = title_match.group(1).strip()
                extracted_model = title_match.group(2).strip()
        
        # GiÃ¡ - tÃ¬m trong title hoáº·c giÃ¡ riÃªng
        price = None
        price_tag = soup.find(class_=re.compile(r'price|gia', re.I))
        if price_tag:
            price = extract_price(price_tag.get_text())
        if not price and title:
            price = extract_price(title)
        
        # Description - mÃ´ táº£ chi tiáº¿t
        description = None
        desc_selectors = [
            ".car-description",
            ".detail-content", 
            ".description",
            "#car-description",
            "[class*='desc']"
        ]
        for selector in desc_selectors:
            desc_tag = soup.select_one(selector)
            if desc_tag:
                description = clean_text(desc_tag.get_text())
                break
        
        # ThÃ´ng tin chi tiáº¿t - tá»« báº£ng thÃ´ng sá»‘
        info = {
            "mileage": None,
            "location": None,
            "year": None,
            "fuel": None,
            "gearbox": None,
            "body": None,
            "color": None,
            "seats": None,
            "engine_power": None,
            "origin": None
        }
        
        # TÃ¬m box_car_detail chá»©a thÃ´ng sá»‘ ká»¹ thuáº­t
        detail_box = soup.find('div', class_='box_car_detail')
        if detail_box:
            # TÃ¬m táº¥t cáº£ cÃ¡c row (div.row, div.row_last) chá»©a thÃ´ng tin
            info_rows = detail_box.find_all('div', class_=re.compile(r'row'))
            
            for row in info_rows:
                text = row.get_text().lower()
                
                # Mileage - km Ä‘Ã£ Ä‘i (xá»­ lÃ½ cáº£ "váº¡n km" vÃ  "km" thÆ°á»ng)
                if re.search(r'(Ä‘Ã£ Ä‘i|sá»‘ km|km Ä‘i)', text) and not info["mileage"]:
                    # Pattern 1: "5 váº¡n km" hoáº·c "5.5 váº¡n km" -> 50000 km
                    match_van = re.search(r'([\d,\.]+)\s*váº¡n\s*km', text)
                    if match_van:
                        num_str = match_van.group(1).replace(',', '.').replace(' ', '')
                        try:
                            num = float(num_str) * 10000  # 1 váº¡n = 10,000
                            info["mileage"] = f"{int(num)} km"
                            continue
                        except:
                            pass
                    
                    # Pattern 2: "50,000 km" hoáº·c "50.000 km" -> 50000 km
                    match_km = re.search(r'([\d,\.]+)\s*km', text)
                    if match_km:
                        num_str = match_km.group(1).replace(',', '').replace('.', '')
                        try:
                            info["mileage"] = f"{int(num_str)} km"
                        except:
                            pass
                
                # Year
                if re.search(r'nÄƒm sáº£n xuáº¥t|nÄƒm sx', text) and not info["year"]:
                    year_match = re.search(r'20\d{2}', text)
                    if year_match:
                        info["year"] = year_match.group()
                
                # Engine (Äá»™ng cÆ¡) - láº¥y toÃ n bá»™ text sau "Äá»™ng cÆ¡:"
                if re.search(r'Ä‘á»™ng cÆ¡', text) and not info.get("engine"):
                    # TÃ¬m span.inp chá»©a thÃ´ng tin Ä‘á»™ng cÆ¡
                    inp_span = row.find('span', class_='inp')
                    if inp_span:
                        engine_text = clean_text(inp_span.get_text())
                        if engine_text:
                            info["engine"] = engine_text
                            # TrÃ­ch xuáº¥t fuel tá»« engine (XÄƒng/Dáº§u/Äiá»‡n/Hybrid)
                            if not info["fuel"]:
                                if 'xÄƒng' in engine_text.lower():
                                    info["fuel"] = "XÄƒng"
                                elif 'dáº§u' in engine_text.lower() or 'diesel' in engine_text.lower():
                                    info["fuel"] = "Dáº§u"
                                elif 'Ä‘iá»‡n' in engine_text.lower():
                                    info["fuel"] = "Äiá»‡n"
                                elif 'hybrid' in engine_text.lower():
                                    info["fuel"] = "Hybrid"
                
                # Gearbox (Há»™p sá»‘)
                if re.search(r'há»™p sá»‘', text) and not info["gearbox"]:
                    if 'tá»± Ä‘á»™ng' in text or 'cvt' in text:
                        info["gearbox"] = "Tá»± Ä‘á»™ng"
                    elif 'sá»‘ tay' in text or 'sá»‘ sÃ n' in text or 'thá»§ cÃ´ng' in text:
                        info["gearbox"] = "Sá»‘ sÃ n"
                
                # Body type (Kiá»ƒu dÃ¡ng)
                if re.search(r'kiá»ƒu dÃ¡ng|loáº¡i xe', text) and not info["body"]:
                    inp_span = row.find('span', class_='inp')
                    if inp_span:
                        body_text = clean_text(inp_span.get_text()).lower()
                        if 'sedan' in body_text:
                            info["body"] = "Sedan"
                        elif 'suv' in body_text:
                            info["body"] = "SUV"
                        elif 'hatchback' in body_text:
                            info["body"] = "Hatchback"
                        elif 'crossover' in body_text:
                            info["body"] = "Crossover"
                        elif 'mpv' in body_text:
                            info["body"] = "MPV"
                
                # Color (MÃ u ngoáº¡i tháº¥t)
                if re.search(r'mÃ u ngoáº¡i tháº¥t', text) and not info["color"]:
                    inp_span = row.find('span', class_='inp')
                    if inp_span:
                        color_text = clean_text(inp_span.get_text())
                        if color_text:
                            info["color"] = color_text.title()
                
                # Seats (Sá»‘ chá»— ngá»“i)
                if re.search(r'sá»‘ chá»—|chá»— ngá»“i', text) and not info["seats"]:
                    seats_match = re.search(r'(\d+)\s*(gháº¿|chá»—)', text)
                    if seats_match:
                        info["seats"] = seats_match.group(1)
                
                # Origin (Xuáº¥t xá»©)
                if re.search(r'xuáº¥t xá»©', text) and not info["origin"]:
                    inp_span = row.find('span', class_='inp')
                    if inp_span:
                        origin_text = clean_text(inp_span.get_text()).lower()
                        if 'nháº­t' in origin_text:
                            info["origin"] = "Nháº­t Báº£n"
                        elif 'thÃ¡i' in origin_text:
                            info["origin"] = "ThÃ¡i Lan"
                        elif 'hÃ n' in origin_text:
                            info["origin"] = "HÃ n Quá»‘c"
                        elif 'viá»‡t' in origin_text or 'trong nÆ°á»›c' in origin_text or 'láº¯p rÃ¡p' in origin_text:
                            info["origin"] = "Viá»‡t Nam"
        
        # Láº¥y nÄƒm tá»« title náº¿u chÆ°a cÃ³
        if not info["year"] and title:
            year_match = re.search(r'20\d{2}', title)
            if year_match:
                info["year"] = year_match.group()
        
        # Location - tá»« contact-box
        contact_box = soup.find('div', class_='contact-box')
        if contact_box:
            contact_txt = contact_box.find('div', class_='contact-txt')
            if contact_txt:
                # TÃ¬m dÃ²ng "Äá»‹a chá»‰:"
                text = contact_txt.get_text()
                addr_match = re.search(r'Äá»‹a chá»‰:\s*(.+?)(?:\n|Website|$)', text, re.I)
                if addr_match:
                    addr = clean_text(addr_match.group(1))
                    # TrÃ­ch xuáº¥t tá»‰nh/thÃ nh phá»‘ (tá»« cuá»‘i cÃ¹ng)
                    parts = addr.split(',')
                    if parts:
                        info["location"] = clean_text(parts[-1])  # Láº¥y pháº§n cuá»‘i cÃ¹ng (tá»‰nh/TP)
        
        # Accident free & single owner tá»« description
        page_text = soup.get_text().lower()
        accident_free = None
        if any(kw in page_text for kw in ['khÃ´ng tai náº¡n', 'ko tai náº¡n', 'khÃ´ng Ä‘Ã¢m Ä‘á»¥ng', 'ko Ä‘Ã¢m', 'chÆ°a Ä‘Ã¢m', 'accident free']):
            accident_free = True
        elif any(kw in page_text for kw in ['tai náº¡n', 'Ä‘Ã¢m Ä‘á»¥ng', 'accident']):
            accident_free = False
        
        single_owner = any(kw in page_text for kw in ['1 chá»§', 'má»™t chá»§', 'single owner', 'chá»§ duy nháº¥t', 'chá»§ tá»« Ä‘áº§u', 'chá»§ tá»« má»›i'])
        
        return {
            "title": title,
            "price": price,
            "mileage": info["mileage"],
            "location": info["location"],
            "year": info["year"],
            "fuel": info["fuel"],
            "engine": info.get("engine"),  # ThÃ´ng tin Ä‘á»™ng cÆ¡ Ä‘áº§y Ä‘á»§: "XÄƒng 1.5 L"
            "gearbox": info["gearbox"],
            "body": info["body"],
            "color": info["color"],
            "seats": info["seats"],
            "engine_power": info["engine_power"],
            "origin": info["origin"],
            "accident_free": accident_free,
            "single_owner": single_owner,
            "description": description,
            "url": url,
            "extracted_make": extracted_make,
            "extracted_model": extracted_model
        }
    except Exception as e:
        print(f"  âŒ Error fetching {url}: {e}")
        return None

def get_all_models_for_brand(brand, headers):
    """Láº¥y danh sÃ¡ch táº¥t cáº£ cÃ¡c model cá»§a má»™t hÃ£ng xe"""
    try:
        url = f"https://bonbanh.com/oto/{brand}"
        res = requests.get(url, timeout=15, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        
        models = set()
        # TÃ¬m táº¥t cáº£ cÃ¡c link cÃ³ dáº¡ng /oto/brand-model
        for link in soup.find_all("a", href=True):
            href = link["href"]
            # Pattern: /oto/brand-model hoáº·c /oto/brand-model-something
            pattern = f"^/oto/{brand}-([a-z0-9-]+)$"
            match = re.match(pattern, href)
            if match:
                model = match.group(1)
                # Loáº¡i bá» cÃ¡c suffix khÃ´ng pháº£i model
                if not any(x in model for x in ['cu', 'moi', 'nam-', 'mau-', 'so-']):
                    models.add(model)
        
        return list(models)
    except Exception as e:
        print(f"  âŒ Error getting models for {brand}: {e}")
        return []

def scrape_listings(make, model, max_pages, csv_writer, scraped_urls):
    """Scrape listings cho má»™t model cá»¥ thá»ƒ"""
    base_url = f"https://bonbanh.com/oto/{make}-{model}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    count = 0
    for page in range(1, max_pages + 1):
        url_page = f"{base_url}?page={page}" if page > 1 else base_url
        print(f"    ğŸ“„ Page {page}...")
        
        try:
            res = requests.get(url_page, timeout=15, headers=headers)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            
            # TÃ¬m danh sÃ¡ch xe
            car_links = []
            
            # TÃ¬m theo class car-item
            for item in soup.select("li.car-item, div.car-item, .car-item"):
                link = item.find("a", href=True)
                if link:
                    car_links.append(link["href"])
            
            # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ tÃ¬m táº¥t cáº£ link cÃ³ pattern /xe-
            if not car_links:
                for link in soup.find_all("a", href=re.compile(r'^/xe-')):
                    car_links.append(link["href"])
            
            if not car_links:
                print(f"    âš ï¸ No cars found on page {page}")
                break
            
            print(f"    ğŸ” Found {len(car_links)} cars on page {page}")
            
            for href in car_links:
                url_full = urljoin("https://bonbanh.com", href)
                ad_id = extract_ad_id_from_url(url_full)
                
                # Kiá»ƒm tra duplicate
                if url_full in scraped_urls:
                    continue
                
                scraped_urls.add(url_full)
                
                # Láº¥y chi tiáº¿t xe
                details = get_car_details(url_full, headers)
                if not details:
                    continue
                
                # Ghi vÃ o CSV
                csv_writer.writerow({
                    "ad_id": ad_id,
                    "make": make,
                    "model": model,
                    "title": details["title"],
                    "price_vnd": details["price"],
                    "mileage": details["mileage"],
                    "location": details["location"],
                    "year": details["year"],
                    "fuel": details["fuel"],
                    "gearbox": details["gearbox"],
                    "body": details["body"],
                    "color": details["color"],
                    "seats": details["seats"],
                    "engine_power": details["engine_power"],
                    "origin": details["origin"],
                    "accident_free": details["accident_free"],
                    "single_owner": details["single_owner"],
                    "description": details["description"],
                    "url": details["url"]
                })
                
                count += 1
                title_short = (details['title'][:50] + '...') if details['title'] and len(details['title']) > 50 else details['title']
                print(f"    âœ… [{count}] {title_short} | {details['price']} triá»‡u")
                
                # Delay ngáº«u nhiÃªn
                time.sleep(random.uniform(1, 3))
                
        except Exception as e:
            print(f"    âŒ Error on page {page}: {e}")
            time.sleep(random.uniform(2, 3))
    
    return count

# ----------------- RUN ----------------- 
if __name__ == "__main__":
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = os.path.join(OUTPUT_DIR, f"cars_data_{timestamp}.csv")
    
    fieldnames = [
        "ad_id", "make", "model", "title", "price_vnd", "mileage", "location",
        "year", "fuel", "gearbox", "body", "color", "seats", "engine_power",
        "origin", "accident_free", "single_owner", "description", "url"
    ]
    
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        scraped_urls = set()
        total_cars = 0
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        print(f"ğŸš€ Báº¯t Ä‘áº§u scrape {len(BRANDS)} hÃ£ng xe")
        print(f"ğŸ“ File output: {csv_filename}\n")
        
        for brand_idx, brand in enumerate(BRANDS, 1):
            print(f"\n{'='*70}")
            print(f"ğŸš— [{brand_idx}/{len(BRANDS)}] HÃ£ng: {brand.upper()}")
            print(f"{'='*70}")
            
            # Láº¥y danh sÃ¡ch models
            print(f"  ğŸ“‹ Äang tÃ¬m danh sÃ¡ch models...")
            models = get_all_models_for_brand(brand, headers)
            
            if not models:
                print(f"  âš ï¸ KhÃ´ng tÃ¬m tháº¥y model nÃ o cho {brand}")
                continue
            
            print(f"  âœ“ TÃ¬m tháº¥y {len(models)} models")
            
            for model_idx, model in enumerate(models, 1):
                print(f"\n  ğŸ”§ [{model_idx}/{len(models)}] Model: {model}")
                count = scrape_listings(brand, model, MAX_PAGES_PER_MODEL, writer, scraped_urls)
                total_cars += count
                print(f"  ğŸ“Š ÄÃ£ scrape {count} xe cho {brand} {model}")
                
                # Delay giá»¯a cÃ¡c model
                time.sleep(random.uniform(2, 4))
    
    print(f"\n{'='*70}")
    print(f"ğŸ‰ HOÃ€N Táº¤T!")
    print(f"{'='*70}")
    print(f"ğŸ“Š Tá»•ng sá»‘ xe thu tháº­p: {total_cars}")
    print(f"ğŸ’¾ Dá»¯ liá»‡u Ä‘Ã£ lÆ°u vÃ o: {csv_filename}")