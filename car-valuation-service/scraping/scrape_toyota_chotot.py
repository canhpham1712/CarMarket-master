"""
Script scrape d·ªØ li·ªáu Toyota t·ª´ chotot.com
S·ª≠ d·ª•ng itemprop (structured data) ƒë·ªÉ extract ch√≠nh x√°c c√°c tr∆∞·ªùng d·ªØ li·ªáu
Tu√¢n th·ªß robots.txt: Allow: / (kh√¥ng d√πng c√°c tham s·ªë filter b·ªã c·∫•m)
"""
import os
import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import random
import csv
from datetime import datetime
from pathlib import Path
import warnings
from contextlib import redirect_stderr, redirect_stdout
from io import StringIO

# Suppress warnings t·ª´ BeautifulSoup v√† requests
warnings.filterwarnings('ignore')

# Suppress stderr/stdout khi parse HTML ƒë·ªÉ tr√°nh spam output
class SuppressOutput:
    """Context manager ƒë·ªÉ suppress stdout/stderr"""
    def __enter__(self):
        self._stdout = sys.stdout
        self._stderr = sys.stderr
        sys.stdout = StringIO()
        sys.stderr = StringIO()
        return self
    
    def __exit__(self, *args):
        sys.stdout = self._stdout
        sys.stderr = self._stderr

# ----------------- C·∫§U H√åNH ----------------- 
OUTPUT_DIR = Path(__file__).resolve().parents[1] / "data"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

MAX_PAGES = 50  # Gi·ªõi h·∫°n s·ªë trang
DELAY_BETWEEN_REQUESTS = (2, 4)  # Delay 2-4 gi√¢y ƒë·ªÉ tu√¢n th·ªß robots.txt

# URL base cho Toyota tr√™n chotot (xe.chotot.com) - scrape theo khu v·ª±c
# C√°c khu v·ª±c ch√≠nh ƒë·ªÉ scrape
REGIONS = [
    "tp-ho-chi-minh-sdcb2",  # TP.HCM
    "ha-noi-sdcb2",          # H√† N·ªôi
    "da-nang-sdcb2",         # ƒê√† N·∫µng
]

# URL ƒë·∫ßy ƒë·ªß cho t·ª´ng khu v·ª±c
REGION_URLS = {
    "tp-ho-chi-minh-sdcb2": "https://xe.chotot.com/mua-ban-oto-toyota-tp-ho-chi-minh-sdcb2",
    "ha-noi-sdcb2": "https://xe.chotot.com/mua-ban-oto-toyota-ha-noi-sdcb2",
    "da-nang-sdcb2": "https://xe.chotot.com/mua-ban-oto-toyota-da-nang-sdcb2",
}

BASE_URL_TEMPLATE = "https://xe.chotot.com/mua-ban-oto-toyota-{region}"

# ----------------- H√ÄM H·ªñ TR·ª¢ ----------------- 
def extract_ad_id_from_url(url):
    """T·∫°o ad_id t·ª´ URL chotot"""
    path = urlparse(url).path
    # URL chotot th∆∞·ªùng c√≥ d·∫°ng: /mua-ban-oto-toyota-vios-xxx.htm ho·∫∑c /mua-ban-oto/xxx.htm
    ad_id = path.strip('/').split('/')[-1].replace('.htm', '')
    return ad_id

def clean_text(text):
    """L√†m s·∫°ch text"""
    if not text:
        return None
    return ' '.join(str(text).split()).strip()

def extract_price_vnd(text):
    """Tr√≠ch xu·∫•t gi√° theo tri·ªáu VND t·ª´ text (v√≠ d·ª•: "435.000.000 ƒë" -> 435)"""
    if not text:
        return None
    
    text = str(text).lower()
    
    # Pattern 1: "1 t·ª∑ 200 tri·ªáu" ho·∫∑c "1 t·ª∑ 420 tri·ªáu"
    match = re.search(r'(\d+(?:\.\d+)?)\s*t·ª∑\s*(\d+)?\s*tri·ªáu?', text)
    if match:
        ty = float(match.group(1))
        tr = int(match.group(2)) if match.group(2) else 0
        return int(ty * 1000 + tr)
    
    # Pattern 2: "1.42 t·ª∑" ho·∫∑c "1 t·ª∑"
    match = re.search(r'(\d+(?:\.\d+)?)\s*t·ª∑', text)
    if match:
        return int(float(match.group(1)) * 1000)
    
    # Pattern 3: "250 tri·ªáu" ho·∫∑c "420 tri·ªáu"
    match = re.search(r'(\d+)\s*tri·ªáu', text)
    if match:
        return int(match.group(1))
    
    # Pattern 4: "435.000.000 ƒë" ho·∫∑c "500.000.000"
    # Lo·∫°i b·ªè d·∫•u ch·∫•m v√† kho·∫£ng tr·∫Øng
    text_clean = text.replace('.', '').replace(',', '').replace(' ', '').replace('ƒë', '')
    
    # T√¨m s·ªë
    match = re.search(r'(\d+)', text_clean)
    if match:
        price_vnd = int(match.group(1))
        # Chuy·ªÉn t·ª´ VND sang tri·ªáu VND
        price_million = price_vnd // 1000000
        return price_million if price_million > 0 else None
    
    return None

def extract_mileage_from_text(text):
    """Tr√≠ch xu·∫•t s·ªë km t·ª´ text, tr·∫£ v·ªÅ format "XXXXX km" """
    if not text:
        return None
    
    text = str(text).lower()
    
    # Pattern 1: "5 v·∫°n km" ho·∫∑c "5.5 v·∫°n km" -> 50000 km
    match_van = re.search(r'([\d,\.]+)\s*v·∫°n\s*km', text)
    if match_van:
        num_str = match_van.group(1).replace(',', '.').replace(' ', '')
        try:
            num = float(num_str) * 10000
            return f"{int(num)} km"
        except:
            pass
    
    # Pattern 2: "50,000 km" ho·∫∑c "50.000 km" ho·∫∑c "50000 km"
    match_km = re.search(r'([\d,\.]+)\s*km', text)
    if match_km:
        num_str = match_km.group(1).replace(',', '').replace('.', '')
        try:
            return f"{int(num_str)} km"
        except:
            pass
    
    # Pattern 3: Ch·ªâ c√≥ s·ªë (kh√¥ng c√≥ "km") - gi·∫£ ƒë·ªãnh l√† km n·∫øu s·ªë l·ªõn h∆°n 1000
    match_num = re.search(r'(\d{4,})', text)
    if match_num:
        num_str = match_num.group(1)
        try:
            num = int(num_str)
            # N·∫øu s·ªë >= 1000, gi·∫£ ƒë·ªãnh l√† km
            if num >= 1000:
                return f"{num} km"
        except:
            pass
    
    return None

def parse_color_from_text(text):
    """Parse m√†u s·∫Øc t·ª´ text"""
    if not text:
        return None
    
    text_lower = str(text).lower()
    color_map = {
        'tr·∫Øng': 'Tr·∫Øng',
        'ƒëen': 'ƒêen',
        'b·∫°c': 'B·∫°c',
        'x√°m': 'X√°m',
        'ghi': 'Ghi',
        'ƒë·ªè': 'ƒê·ªè',
        'xanh': 'Xanh',
        'v√†ng': 'V√†ng',
        'c√°t': 'C√°t',
        'n√¢u': 'N√¢u',
        'b·∫°ch kim': 'B·∫°ch kim',
        'xanh d∆∞∆°ng': 'Xanh d∆∞∆°ng',
        'xanh l√°': 'Xanh l√°',
    }
    
    for key, value in color_map.items():
        if key in text_lower:
            return value
    
    return None

def extract_version_from_title(title, make, model):
    """
    Extract version t·ª´ title
    Format chotot th∆∞·ªùng: "{Make} {Model} {Version} {Year} - {Mileage}" ho·∫∑c "{Version} {Year} - {Mileage}"
    
    Examples:
    - "2 54 2013 2.5G - 127000 km" -> "2.5G"
    - "Toyota Camry 2.5G 2013 - 127000 km" -> "2.5G"
    - "Toyota Vios 2023 1.5G 5390 km" -> "1.5G"
    - "B√°n Toyota Corolla 2009 Japan ch√≠nh ch·ªß cavet" -> None (kh√¥ng c√≥ version r√µ r√†ng)
    """
    if not title:
        return None
    
    title = str(title).strip()
    
    # Pattern 1: "Toyota Model Version Year" ho·∫∑c "Model Version Year"
    if make and model:
        make_str = str(make).strip()
        model_str = str(model).strip()
        
        # Pattern: Make Model Version Year (c√≥ th·ªÉ c√≥ "B√°n" ·ªü ƒë·∫ßu)
        # V√≠ d·ª•: "B√°n Toyota Corolla 2009" ho·∫∑c "Toyota Vios 2023 1.5G"
        pattern = rf"(?:B√°n\s+)?{re.escape(make_str)}\s+{re.escape(model_str)}\s+(.+?)\s+(20\d{{2}})"
        match = re.search(pattern, title, re.IGNORECASE)
        
        if match:
            version = match.group(1).strip()
            # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt ·ªü cu·ªëi
            version = re.sub(r'\s*-\s*$', '', version)  # Lo·∫°i b·ªè " -" ·ªü cu·ªëi
            # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng ph·∫£i version (nh∆∞ "Japan", "ch√≠nh ch·ªß", etc.)
            version = re.sub(r'\s+(Japan|ch√≠nh ch·ªß|cavet|ch·ªß|m√†u|xe|ch·ªó|s·ªë|t·ª± ƒë·ªông|ch·∫°y|xƒÉng|km).*$', '', version, flags=re.IGNORECASE)
            version = version.strip()
            # Ch·ªâ tr·∫£ v·ªÅ n·∫øu c√≥ ch·ª©a s·ªë ho·∫∑c ch·ªØ c√°i (kh√¥ng ph·∫£i ch·ªâ l√† t·ª´ th√¥ng th∆∞·ªùng)
            if version and (re.search(r'\d', version) or len(version) <= 10):
                return version if version else None
        
        # Pattern 1b: "Toyota Model Version" (kh√¥ng c√≥ nƒÉm, c√≥ th·ªÉ trong description)
        # V√≠ d·ª•: "Toyota Vios 1.5G m√†u N√¢u v√†ng"
        pattern = rf"{re.escape(make_str)}\s+{re.escape(model_str)}\s+([\d\.]+\s*[A-Z]+(?:\s+[A-Z]+)?)"
        match = re.search(pattern, title, re.IGNORECASE)
        if match:
            version = match.group(1).strip()
            # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng ph·∫£i version ·ªü sau
            version = re.sub(r'\s+(m√†u|xe|ch·ªó|s·ªë|t·ª± ƒë·ªông|ch·∫°y|xƒÉng|km|Japan|ch√≠nh ch·ªß|cavet|ch·ªß|B√°n).*$', '', version, flags=re.IGNORECASE)
            version = version.strip()
            if version and (re.search(r'\d', version) or len(version) <= 15):
                return version
    
    # Pattern 2: Format "2 54 2013 2.5G - 127000 km" (c√≥ s·ªë ·ªü ƒë·∫ßu)
    # Lo·∫°i b·ªè c√°c s·ªë ƒë∆°n l·∫ª ·ªü ƒë·∫ßu tr∆∞·ªõc
    title_clean = re.sub(r'^\d+\s+\d+\s+', '', title)  # Lo·∫°i b·ªè "2 54 "
    title_clean = re.sub(r'^\d+\s+', '', title_clean)  # Lo·∫°i b·ªè s·ªë ƒë∆°n l·∫ª ·ªü ƒë·∫ßu
    
    # T√¨m pattern: Version Year
    pattern = r'(.+?)\s+(20\d{2})\s*-'
    match = re.search(pattern, title_clean)
    if match:
        before_year = match.group(1).strip()
        
        # Lo·∫°i b·ªè Make v√† Model n·∫øu c√≥
        if make and model:
            make_model_pattern = rf"{re.escape(make)}\s+{re.escape(model)}\s+"
            before_year = re.sub(make_model_pattern, '', before_year, flags=re.IGNORECASE)
        
        version = before_year.strip()
        # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng ph·∫£i version
        version = re.sub(r'\s+(Japan|ch√≠nh ch·ªß|cavet|ch·ªß|B√°n|m√†u|xe|ch·ªó|s·ªë|t·ª± ƒë·ªông|ch·∫°y|xƒÉng|km).*$', '', version, flags=re.IGNORECASE)
        version = version.strip()
        
        # Ch·ªâ tr·∫£ v·ªÅ n·∫øu c√≥ ch·ª©a s·ªë ho·∫∑c l√† version ng·∫Øn (nh∆∞ "1.5G", "2.5G")
        if version and (re.search(r'\d', version) or len(version) <= 10):
            return version if version else None
    
    # Pattern 3: T√¨m pattern s·ªë + ch·ªØ c√°i ngay tr∆∞·ªõc nƒÉm (nh∆∞ "1.5G 2023")
    pattern = r'([\d\.]+\s*[A-Z]+(?:\s+[A-Z]+)?)\s+(20\d{2})'
    match = re.search(pattern, title, re.IGNORECASE)
    if match:
        version = match.group(1).strip()
        return version
    
    # Pattern 4: T√¨m pattern s·ªë + ch·ªØ c√°i ƒë∆°n gi·∫£n (nh∆∞ "1.5G", "2.5G") trong text
    # Ch·ªâ t√¨m n·∫øu c√≥ make v√† model tr∆∞·ªõc ƒë√≥
    if make and model:
        # T√¨m pattern: s·ªë.ch·ªØ ho·∫∑c s·ªë ch·ªØ (nh∆∞ "1.5G", "2.5G", "1.5 E")
        pattern = r'([\d\.]+\s*[A-Z]+(?:\s+[A-Z]+)?)'
        matches = re.findall(pattern, title)
        for match in matches:
            version_candidate = match.strip()
            # Ki·ªÉm tra xem c√≥ ph·∫£i version kh√¥ng (c√≥ s·ªë v√† ch·ªØ, ƒë·ªô d√†i h·ª£p l√Ω)
            if re.search(r'\d', version_candidate) and len(version_candidate) <= 15:
                # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng ph·∫£i version
                version_candidate = re.sub(r'\s+(m√†u|xe|ch·ªó|s·ªë|t·ª± ƒë·ªông|ch·∫°y|xƒÉng|km|Japan|ch√≠nh ch·ªß|cavet|ch·ªß|B√°n).*$', '', version_candidate, flags=re.IGNORECASE)
                version_candidate = version_candidate.strip()
                if version_candidate:
                    return version_candidate
    
    return None

def get_car_details(url, headers):
    """L·∫•y chi ti·∫øt 1 xe t·ª´ trang chi ti·∫øt chotot s·ª≠ d·ª•ng itemprop
    Returns: (details_dict, error_type) ho·∫∑c (None, None) n·∫øu th√†nh c√¥ng
    error_type: '410' cho Gone, 'other' cho l·ªói kh√°c, None n·∫øu th√†nh c√¥ng
    """
    try:
        # Lo·∫°i b·ªè fragment (#px=...) t·ª´ URL
        url_clean = url.split('#')[0] if '#' in url else url
        
        res = requests.get(url_clean, timeout=15, headers=headers)
        
        # X·ª≠ l√Ω ri√™ng l·ªói 410 (Gone) - kh√¥ng raise ƒë·ªÉ c√≥ th·ªÉ return error type
        if res.status_code == 410:
            return None, '410'
        
        res.raise_for_status()
        
        # Lo·∫°i b·ªè script tags ch·ª©a JSON data l·ªõn ƒë·ªÉ tr√°nh spam output
        html_content = res.text
        # Lo·∫°i b·ªè script tags ch·ª©a __NEXT_DATA__ ho·∫∑c JSON data l·ªõn
        html_content = re.sub(r'<script[^>]*id="__NEXT_DATA__"[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<script[^>]*type="application/json"[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        
        # Parse HTML v·ªõi suppress output ƒë·ªÉ tr√°nh spam
        with SuppressOutput():
            soup = BeautifulSoup(html_content, "html.parser")
        
        # Title - t·ª´ h1
        title = None
        h1_tag = soup.find("h1")
        if h1_tag:
            title = clean_text(h1_tag.get_text())
        
        # Make - t·ª´ itemprop="carbrand"
        make = None
        make_tag = soup.find(itemprop="carbrand")
        if make_tag:
            make = clean_text(make_tag.get_text())
        elif title:
            # Fallback: parse t·ª´ title
            if 'toyota' in title.lower():
                make = "Toyota"
        
        # Model - t·ª´ itemprop="carmodel"
        model = None
        model_tag = soup.find(itemprop="carmodel")
        if model_tag:
            model = clean_text(model_tag.get_text())
        
        # Year - t·ª´ itemprop="mfdate"
        year = None
        year_tag = soup.find(itemprop="mfdate")
        if year_tag:
            year = clean_text(year_tag.get_text())
        
        # Mileage - t·ª´ itemprop="mileage_v2"
        mileage = None
        mileage_tag = soup.find(itemprop="mileage_v2")
        if mileage_tag:
            mileage_value = clean_text(mileage_tag.get_text())
            # C√≥ th·ªÉ l√† s·ªë thu·∫ßn ho·∫∑c c√≥ "km"
            if mileage_value:
                mileage = extract_mileage_from_text(mileage_value) or f"{mileage_value} km"
        
        # Fuel - t·ª´ itemprop="fuel"
        fuel = None
        fuel_tag = soup.find(itemprop="fuel")
        if fuel_tag:
            fuel_text = clean_text(fuel_tag.get_text()).lower()
            if 'xƒÉng' in fuel_text or 'petrol' in fuel_text:
                fuel = "XƒÉng"
            elif 'd·∫ßu' in fuel_text or 'diesel' in fuel_text:
                fuel = "D·∫ßu"
            elif 'ƒëi·ªán' in fuel_text or 'electric' in fuel_text:
                fuel = "ƒêi·ªán"
            elif 'hybrid' in fuel_text:
                fuel = "Hybrid"
        
        # Gearbox - t·ª´ itemprop="gearbox"
        gearbox = None
        gearbox_tag = soup.find(itemprop="gearbox")
        if gearbox_tag:
            gearbox_text = clean_text(gearbox_tag.get_text()).lower()
            if 't·ª± ƒë·ªông' in gearbox_text or 'automatic' in gearbox_text or 'cvt' in gearbox_text:
                gearbox = "T·ª± ƒë·ªông"
            elif 's·ªë s√†n' in gearbox_text or 's·ªë tay' in gearbox_text or 'manual' in gearbox_text:
                gearbox = "S·ªë s√†n"
        
        # Body - t·ª´ itemprop="cartype"
        body = None
        body_tag = soup.find(itemprop="cartype")
        if body_tag:
            body_text = clean_text(body_tag.get_text()).lower()
            if 'sedan' in body_text:
                body = "Sedan"
            elif 'suv' in body_text:
                body = "SUV"
            elif 'hatchback' in body_text:
                body = "Hatchback"
            elif 'crossover' in body_text:
                body = "Crossover"
            elif 'mpv' in body_text:
                body = "MPV"
            elif 'pickup' in body_text or 'b√°n t·∫£i' in body_text:
                body = "Pickup"
        
        # Seats - t·ª´ itemprop="carseats"
        seats = None
        seats_tag = soup.find(itemprop="carseats")
        if seats_tag:
            seats = clean_text(seats_tag.get_text())
        
        # Origin - t·ª´ itemprop="carorigin"
        origin = None
        origin_tag = soup.find(itemprop="carorigin")
        if origin_tag:
            origin_text = clean_text(origin_tag.get_text()).lower()
            if 'nh·∫≠t' in origin_text or 'japan' in origin_text:
                origin = "Nh·∫≠t B·∫£n"
            elif 'th√°i' in origin_text or 'thailand' in origin_text:
                origin = "Th√°i Lan"
            elif 'h√†n' in origin_text or 'korea' in origin_text:
                origin = "H√†n Qu·ªëc"
            elif 'vi·ªát' in origin_text or 'vietnam' in origin_text or 'trong n∆∞·ªõc' in origin_text:
                origin = "Vi·ªát Nam"
        
        # Engine capacity - t·ª´ itemprop="engine_capacity"
        engine_capacity = None
        engine_cap_tag = soup.find(itemprop="engine_capacity")
        if engine_cap_tag:
            engine_capacity = clean_text(engine_cap_tag.get_text())
        
        # Engine power - t·ª´ itemprop="horse_power"
        engine_power = None
        engine_power_tag = soup.find(itemprop="horse_power")
        if engine_power_tag:
            engine_power = clean_text(engine_power_tag.get_text())
        
        # Fallback: t√¨m trong c√°c div c√≥ label "C√¥ng su·∫•t ƒë·ªông c∆°"
        if not engine_power:
            # T√¨m div ch·ª©a text "C√¥ng su·∫•t ƒë·ªông c∆°" v√† l·∫•y gi√° tr·ªã b√™n c·∫°nh
            for div in soup.find_all('div'):
                text = div.get_text()
                if 'c√¥ng su·∫•t ƒë·ªông c∆°' in text.lower() or 'horse_power' in str(div.get('itemprop', '')).lower():
                    # T√¨m s·ªë HP trong text
                    hp_match = re.search(r'(\d+\s*HP(?:\s*@\s*\d+\s*RPM)?)', text, re.I)
                    if hp_match:
                        engine_power = clean_text(hp_match.group(1))
                        break
                    # Ho·∫∑c l·∫•y text t·ª´ span/value element b√™n c·∫°nh
                    value_elem = div.find_next('span') or div.find(class_=lambda x: x and 'value' in str(x).lower())
                    if value_elem:
                        power_text = clean_text(value_elem.get_text())
                        if power_text and ('HP' in power_text.upper() or 'm√£ l·ª±c' in power_text.lower()):
                            engine_power = power_text
                            break
        
        # Engine - k·∫øt h·ª£p fuel + engine_capacity
        engine = None
        if fuel and engine_capacity:
            engine = f"{fuel} {engine_capacity} L"
        elif fuel:
            engine = fuel
        
        # Price - t√¨m trong c√°c element c√≥ class ch·ª©a "price"
        price_vnd = None
        price_selectors = [
            soup.find(class_=lambda x: x and 'price' in str(x).lower()),
            soup.find('b', class_=lambda x: x and 'price' in str(x).lower()),
            soup.find(string=re.compile(r'\d+\.?\d*\.?\d*\.?\d*\s*ƒë', re.I)),
        ]
        
        for price_elem in price_selectors:
            if price_elem:
                if isinstance(price_elem, str):
                    price_vnd = extract_price_vnd(price_elem)
                else:
                    price_vnd = extract_price_vnd(price_elem.get_text())
                if price_vnd:
                    break
        
        # Location - t√¨m trong c√°c element c√≥ ƒë·ªãa ch·ªâ
        location = None
        # T√¨m element c√≥ icon location ho·∫∑c text ch·ª©a ƒë·ªãa ch·ªâ
        location_selectors = [
            soup.find(string=re.compile(r'(H√† N·ªôi|TP\.?\s*HCM|ƒê√† N·∫µng|H·∫£i Ph√≤ng|C·∫ßn Th∆°)', re.I)),
            soup.find(class_=lambda x: x and 'location' in str(x).lower()),
        ]
        
        for loc_elem in location_selectors:
            if loc_elem:
                if isinstance(loc_elem, str):
                    location = clean_text(loc_elem)
                else:
                    # L·∫•y text t·ª´ parent element
                    parent = loc_elem.parent if hasattr(loc_elem, 'parent') else None
                    if parent:
                        location = clean_text(parent.get_text())
                if location and len(location) > 5:
                    break
        
        # N·∫øu ch∆∞a c√≥ location, t√¨m trong breadcrumb ho·∫∑c c√°c element kh√°c
        if not location:
            page_text = soup.get_text()
            location_match = re.search(r'(X√£|Ph∆∞·ªùng|Qu·∫≠n|Huy·ªán|Th√†nh ph·ªë|TP\.?)\s+([^,\n]+)', page_text)
            if location_match:
                location = clean_text(location_match.group(0))
        
        # Color - parse t·ª´ description ho·∫∑c title
        color = None
        description_text = ""
        
        # Description - t·ª´ itemprop="description"
        description = None
        desc_tag = soup.find(itemprop="description")
        if desc_tag:
            description = clean_text(desc_tag.get_text())
            description_text = description.lower()
        
        # T√¨m color t·ª´ description ho·∫∑c title
        if description_text:
            color = parse_color_from_text(description_text)
        if not color and title:
            color = parse_color_from_text(title.lower())
        
        # Version - t√¨m theo th·ª© t·ª± ∆∞u ti√™n:
        # 1. itemprop="option" (Phi√™n b·∫£n xe trong th√¥ng s·ªë)
        # 2. Title
        # 3. Description
        version = None
        
        # ∆Øu ti√™n 1: T·ª´ itemprop="option"
        option_tag = soup.find(itemprop="option")
        if option_tag:
            version = clean_text(option_tag.get_text())
            if version and version.strip():
                version = version.strip()
            else:
                version = None
        
        # ∆Øu ti√™n 2: Extract t·ª´ title
        if not version and title and make and model:
            version = extract_version_from_title(title, make, model)
        
        # ∆Øu ti√™n 3: Extract t·ª´ description
        if not version and description and make and model:
            version = extract_version_from_title(description, make, model)
        
        # Accident free & single owner t·ª´ description
        accident_free = None
        single_owner = False
        
        full_text = (description or '').lower() + ' ' + (title or '').lower()
        
        if any(kw in full_text for kw in ['kh√¥ng tai n·∫°n', 'ko tai n·∫°n', 'kh√¥ng ƒë√¢m ƒë·ª•ng', 'ko ƒë√¢m', 'ch∆∞a ƒë√¢m', 'accident free', 'kh√¥ng ng·∫≠p n∆∞·ªõc']):
            accident_free = True
        elif any(kw in full_text for kw in ['tai n·∫°n', 'ƒë√¢m ƒë·ª•ng', 'accident']):
            accident_free = False
        
        single_owner = any(kw in full_text for kw in ['1 ch·ªß', 'm·ªôt ch·ªß', 'single owner', 'ch·ªß duy nh·∫•t', 'ch·ªß t·ª´ ƒë·∫ßu', 'ch·ªß t·ª´ m·ªõi'])
        
        return {
            "title": title,
            "make": make,
            "model": model,
            "version": version,
            "price_vnd": price_vnd,
            "mileage": mileage,
            "location": location,
            "year": year,
            "fuel": fuel,
            "engine": engine,
            "gearbox": gearbox,
            "body": body,
            "color": color,
            "seats": seats,
            "engine_power": engine_power,
            "origin": origin,
            "accident_free": accident_free,
            "single_owner": single_owner,
            "description": description,
            "url": url_clean  # URL ƒë√£ lo·∫°i b·ªè fragment
        }
        return details, None  # Th√†nh c√¥ng
    except requests.exceptions.HTTPError as e:
        # X·ª≠ l√Ω l·ªói HTTP
        if e.response and e.response.status_code == 410:
            return None, '410'  # Gone - kh√¥ng in error
        # C√°c l·ªói HTTP kh√°c
        error_msg = str(e)[:200]
        print(f"  ‚ùå Error fetching {url}: {error_msg}")
        return None, 'other'
    except Exception as e:
        # C√°c l·ªói kh√°c (timeout, connection, etc.)
        error_msg = str(e)[:200]  # Gi·ªõi h·∫°n ƒë·ªô d√†i error message
        print(f"  ‚ùå Error fetching {url}: {error_msg}")
        return None, 'other'

def scrape_chotot_listings(region=None, max_pages=MAX_PAGES, scraped_urls=None):
    """Scrape listings t·ª´ chotot.com theo khu v·ª±c"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    cars = []
    if scraped_urls is None:
        scraped_urls = set()
    
    # X√°c ƒë·ªãnh URL base
    if region:
        # S·ª≠ d·ª•ng REGION_URLS n·∫øu c√≥, n·∫øu kh√¥ng th√¨ d√πng template
        base_url = REGION_URLS.get(region) or BASE_URL_TEMPLATE.format(region=region)
    else:
        # M·∫∑c ƒë·ªãnh TP.HCM
        base_url = REGION_URLS.get(REGIONS[0]) or BASE_URL_TEMPLATE.format(region=REGIONS[0])
    
    page = 1
    consecutive_empty_pages = 0
    max_empty_pages = 2
    
    while page <= max_pages:
        # URL v·ªõi pagination
        if page == 1:
            url_page = base_url
        else:
            url_page = f"{base_url}?page={page}"
        
        print(f"\nüìÑ Page {page}: {url_page}")
        
        try:
            res = requests.get(url_page, timeout=15, headers=headers)
            res.raise_for_status()
            
            # L∆∞u HTML g·ªëc ƒë·ªÉ parse
            html_content = res.text
            
            # Parse HTML v·ªõi suppress output ƒë·ªÉ tr√°nh spam
            with SuppressOutput():
                soup = BeautifulSoup(html_content, "html.parser")
            
            # T√¨m c√°c listing links
            car_links = []
            
            # C√°ch 1: T√¨m trong JSON data t·ª´ __NEXT_DATA__ (∆∞u ti√™n cao nh·∫•t)
            try:
                next_data_script = soup.find('script', id='__NEXT_DATA__')
                if next_data_script:
                    import json
                    next_data = json.loads(next_data_script.string)
                    
                    # T√¨m trong props.initialState.adlisting.data.ads (theo k·∫øt qu·∫£ diagnose)
                    def extract_urls_from_json(obj, path=""):
                        """Recursively extract URLs from JSON structure"""
                        urls = []
                        if isinstance(obj, dict):
                            # Ki·ªÉm tra c√°c key c√≥ th·ªÉ ch·ª©a URL
                            for key in ['url', 'link', 'href', 'ad_url', 'adUrl', 'detail_url', 'detailUrl']:
                                if key in obj:
                                    url_value = str(obj[key])
                                    if '/mua-ban-oto' in url_value:
                                        if not url_value.startswith('http'):
                                            url_value = urljoin('https://xe.chotot.com', url_value)
                                        # Lo·∫°i b·ªè fragment
                                        url_value = url_value.split('#')[0]
                                        if url_value not in urls:
                                            urls.append(url_value)
                            
                            # Ki·ªÉm tra c√°c key c√≥ th·ªÉ ch·ª©a list c·ªßa ads
                            for key in ['ads', 'listings', 'items', 'results', 'data', 'sticky_ads', 'stickyAds']:
                                if key in obj:
                                    urls.extend(extract_urls_from_json(obj[key], f"{path}.{key}"))
                            
                            # Recursively search in all values
                            for value in obj.values():
                                urls.extend(extract_urls_from_json(value, path))
                        
                        elif isinstance(obj, list):
                            for item in obj:
                                urls.extend(extract_urls_from_json(item, path))
                        
                        return urls
                    
                    # Extract t·ª´ to√†n b·ªô JSON structure
                    json_urls = extract_urls_from_json(next_data)
                    for url_value in json_urls:
                        # Validate URL c√≥ ID s·ªë
                        if re.search(r'/\d+\.htm', url_value) or re.search(r'/mua-ban-oto[^/]*/\d+', url_value):
                            if url_value not in car_links:
                                car_links.append(url_value)
                    
                    # N·∫øu ch∆∞a t√¨m th·∫•y, th·ª≠ ƒë∆∞·ªùng d·∫´n c·ª• th·ªÉ
                    if not car_links and 'props' in next_data:
                        props = next_data['props']
                        # Th·ª≠ initialState.adlisting.data.ads
                        if 'initialState' in props:
                            initial_state = props['initialState']
                            if 'adlisting' in initial_state:
                                adlisting = initial_state['adlisting']
                                if 'data' in adlisting and 'ads' in adlisting['data']:
                                    ads = adlisting['data']['ads']
                                    if isinstance(ads, list):
                                        for ad in ads:
                                            if isinstance(ad, dict):
                                                # T√¨m URL trong ad object
                                                for url_key in ['url', 'link', 'href', 'ad_url', 'adUrl', 'detail_url', 'detailUrl', 'ad_id']:
                                                    if url_key in ad:
                                                        url_value = str(ad[url_key])
                                                        if '/mua-ban-oto' in url_value or (url_key == 'ad_id' and url_value.isdigit()):
                                                            # N·∫øu l√† ad_id, t·∫°o URL
                                                            if url_key == 'ad_id':
                                                                # C·∫ßn t√¨m category ho·∫∑c t·∫°o URL generic
                                                                url_value = f"/mua-ban-oto/{url_value}.htm"
                                                            if not url_value.startswith('http'):
                                                                url_value = urljoin('https://xe.chotot.com', url_value)
                                                            url_value = url_value.split('#')[0]
                                                            if url_value not in car_links:
                                                                car_links.append(url_value)
                        
                        # Th·ª≠ pageProps
                        if 'pageProps' in props:
                            page_props = props['pageProps']
                            for key in ['listings', 'ads', 'items', 'results', 'data']:
                                if key in page_props and isinstance(page_props[key], list):
                                    for item in page_props[key]:
                                        if isinstance(item, dict):
                                            for url_key in ['url', 'link', 'href', 'ad_id', 'id']:
                                                if url_key in item:
                                                    url_value = str(item[url_key])
                                                    if '/mua-ban-oto' in url_value or (url_key == 'ad_id' and url_value.isdigit()):
                                                        if url_key == 'ad_id':
                                                            url_value = f"/mua-ban-oto/{url_value}.htm"
                                                        if not url_value.startswith('http'):
                                                            url_value = urljoin('https://xe.chotot.com', url_value)
                                                        url_value = url_value.split('#')[0]
                                                        if url_value not in car_links:
                                                            car_links.append(url_value)
            except Exception as e:
                # N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, ti·∫øp t·ª•c v·ªõi c√°ch kh√°c
                pass
            
            # C√°ch 2: T√¨m t·∫•t c·∫£ link c√≥ ch·ª©a "/mua-ban-oto" v√† c√≥ ID s·ªë
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href:
                    # Lo·∫°i b·ªè fragment t·ª´ href
                    href = href.split('#')[0]
                    
                    # URL c√≥ th·ªÉ l√† relative ho·∫∑c absolute
                    if '/mua-ban-oto' in href:
                        # Pattern linh ho·∫°t h∆°n:
                        # - /mua-ban-oto-xxx/123456.htm
                        # - /mua-ban-oto/123456.htm
                        # - /mua-ban-oto-xxx/123456 (kh√¥ng c√≥ .htm)
                        # - /mua-ban-oto/123456 (kh√¥ng c√≥ .htm)
                        if not href.startswith('http'):
                            href = urljoin('https://xe.chotot.com', href)
                        
                        # Ch·∫•p nh·∫≠n URL c√≥ s·ªë ID (c√≥ th·ªÉ c√≥ ho·∫∑c kh√¥ng c√≥ .htm)
                        # Pattern: c√≥ s·ªë sau d·∫•u / cu·ªëi c√πng
                        if re.search(r'/\d+\.htm', href) or re.search(r'/mua-ban-oto[^/]*/\d+', href) or re.search(r'/mua-ban-oto.*/\d+$', href):
                            if href not in car_links:
                                car_links.append(href)
            
            # C√°ch 3: T√¨m trong c√°c card/listing element v·ªõi nhi·ªÅu selector kh√°c nhau
            selectors = [
                {'class': lambda x: x and any(kw in str(x).lower() for kw in ['card', 'ad', 'item', 'listing', 'product'])},
                {'data-testid': lambda x: x and any(kw in str(x).lower() for kw in ['ad', 'listing', 'card'])},
                {'class': lambda x: x and 'ad-item' in str(x).lower()},
                {'class': lambda x: x and 'listing-item' in str(x).lower()},
            ]
            
            for selector in selectors:
                for elem in soup.find_all(attrs=selector):
                    # T√¨m link trong element v√† c√°c children
                    links = elem.find_all('a', href=True)
                    for link in links:
                        href = link.get('href')
                        if href:
                            href = href.split('#')[0]
                            if '/mua-ban-oto' in href:
                                if not href.startswith('http'):
                                    href = urljoin('https://xe.chotot.com', href)
                                # Pattern linh ho·∫°t: c√≥ s·ªë ID
                                if re.search(r'/\d+\.htm', href) or re.search(r'/mua-ban-oto[^/]*/\d+', href) or re.search(r'/mua-ban-oto.*/\d+$', href):
                                    if href not in car_links:
                                        car_links.append(href)
            
            # C√°ch 4: T√¨m trong t·∫•t c·∫£ c√°c link c√≥ pattern chotot (fallback)
            for link in soup.find_all('a', href=re.compile(r'/mua-ban-oto')):
                href = link.get('href')
                if href:
                    href = href.split('#')[0]
                    if not href.startswith('http'):
                        href = urljoin('https://xe.chotot.com', href)
                    # Ch·∫•p nh·∫≠n c·∫£ .htm v√† kh√¥ng c√≥ .htm nh∆∞ng c√≥ s·ªë ID
                    if re.search(r'/\d+\.htm', href) or re.search(r'/mua-ban-oto[^/]*/\d+', href) or re.search(r'/mua-ban-oto.*/\d+$', href):
                        if href not in car_links:
                            car_links.append(href)
            
            if not car_links:
                consecutive_empty_pages += 1
                print(f"  ‚ö†Ô∏è No cars found on page {page} (empty pages: {consecutive_empty_pages})")
                
                if consecutive_empty_pages >= max_empty_pages:
                    print(f"  ‚úÖ Reached end of listings")
                    break
                
                page += 1
                time.sleep(random.uniform(*DELAY_BETWEEN_REQUESTS))
                continue
            
            consecutive_empty_pages = 0
            print(f"  üîç Found {len(car_links)} cars on page {page}")
            
            # Counter cho c√°c lo·∫°i l·ªói
            error_410_count = 0
            error_other_count = 0
            
            for idx, url_full in enumerate(car_links, 1):
                # Ki·ªÉm tra duplicate (scraped_urls ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ h√†m main)
                if url_full in scraped_urls:
                    continue
                scraped_urls.add(url_full)
                
                ad_id = extract_ad_id_from_url(url_full)
                
                # L·∫•y chi ti·∫øt xe
                details, error_type = get_car_details(url_full, headers)
                
                # ƒê·∫øm l·ªói 410 (kh√¥ng in t·ª´ng error)
                if error_type == '410':
                    error_410_count += 1
                    continue
                
                # ƒê·∫øm c√°c l·ªói kh√°c (ƒë√£ ƒë∆∞·ª£c in trong get_car_details)
                if error_type == 'other':
                    error_other_count += 1
                    continue
                
                if not details:
                    continue
                
                # Ch·ªâ l·∫•y Toyota
                if details.get("make") and details["make"].lower() != "toyota":
                    continue
                
                car_data = {
                    "ad_id": ad_id,
                    "make": details.get("make") or "Toyota",
                    "model": details.get("model"),
                    "version": details.get("version"),
                    "title": details["title"],
                    "price_vnd": details["price_vnd"],
                    "mileage": details["mileage"],
                    "location": details["location"],
                    "year": details["year"],
                    "fuel": details["fuel"],
                    "engine": details.get("engine"),
                    "gearbox": details["gearbox"],
                    "body": details["body"],
                    "color": details["color"],
                    "seats": details["seats"],
                    "engine_power": details["engine_power"],
                    "origin": details["origin"],
                    "accident_free": details["accident_free"],
                    "single_owner": details["single_owner"],
                    "description": details["description"],
                    "url": details["url"]
                }
                
                cars.append(car_data)
                title_short = (details['title'][:50] + '...') if details['title'] and len(details['title']) > 50 else details['title']
                print(f"  ‚úÖ [{len(cars)}] {title_short} | {details['price_vnd']} tri·ªáu")
                
                # Delay gi·ªØa c√°c request
                time.sleep(random.uniform(*DELAY_BETWEEN_REQUESTS))
            
            # In summary c·ªßa errors n·∫øu c√≥
            if error_410_count > 0:
                print(f"  ‚ö†Ô∏è  {error_410_count} listings removed/deleted (410 Gone)")
            if error_other_count > 0:
                print(f"  ‚ö†Ô∏è  {error_other_count} other errors occurred")
            
            # Delay gi·ªØa c√°c trang
            time.sleep(random.uniform(3, 5))
            page += 1
            
        except Exception as e:
            # Ch·ªâ in error message ng·∫Øn g·ªçn
            error_msg = str(e)[:200]  # Gi·ªõi h·∫°n ƒë·ªô d√†i error message
            print(f"  ‚ùå Error on page {page}: {error_msg}")
            time.sleep(random.uniform(3, 5))
            page += 1
            continue
    
    return cars

def main():
    """Main function - scrape t·ª´ t·∫•t c·∫£ c√°c khu v·ª±c"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = OUTPUT_DIR / f"toyota_chotot_{timestamp}.csv"
    
    fieldnames = [
        "ad_id", "make", "model", "version", "title", "price_vnd", "mileage", "location",
        "year", "fuel", "engine", "gearbox", "body", "color", "seats", "engine_power",
        "origin", "accident_free", "single_owner", "description", "url"
    ]
    
    print(f"\nüöÄ Starting Chotot scraper for Toyota")
    print(f"üìã Regions: {', '.join(REGIONS)}")
    print(f"üìã Max pages per region: {MAX_PAGES}")
    print(f"üíæ Output file: {csv_filename}\n")
    
    all_cars = []
    scraped_urls = set()  # D√πng chung ƒë·ªÉ tr√°nh duplicate gi·ªØa c√°c khu v·ª±c
    
    for region_idx, region in enumerate(REGIONS, 1):
        print(f"\n{'='*70}")
        print(f"üåç [{region_idx}/{len(REGIONS)}] Scraping region: {region}")
        print(f"{'='*70}")
        
        cars = scrape_chotot_listings(region=region, max_pages=MAX_PAGES, scraped_urls=scraped_urls)
        
        all_cars.extend(cars)
        print(f"‚úÖ Scraped {len(cars)} cars from {region} (total: {len(all_cars)})")
        
        # Delay gi·ªØa c√°c khu v·ª±c
        if region_idx < len(REGIONS):
            delay = random.uniform(5, 8)
            print(f"‚è≥ Waiting {delay:.1f}s before next region...")
            time.sleep(delay)
    
    cars = all_cars
    
    if cars:
        with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(cars)
        
        print(f"\n{'='*60}")
        print(f"üéâ SCRAPING COMPLETED!")
        print(f"{'='*60}")
        print(f"üìä Total cars scraped: {len(cars)}")
        print(f"üíæ Saved to: {csv_filename}")
        
        # Th·ªëng k√™
        from collections import Counter
        model_counts = Counter(car.get("model") or "Unknown" for car in cars)
        print(f"\nüìà Breakdown by model:")
        for model, count in model_counts.most_common():
            print(f"  - {model}: {count} cars")
    else:
        print("\n‚ö†Ô∏è No cars scraped!")

if __name__ == "__main__":
    main()
